### مدل **BART (Bidirectional and AutoRegressive Transformer)**

**BART یک مدل زبانی encoder-decoder است که از ترکیبی از آموزش دوطرفه (bidirectional) و خودрегرسیونی (autoregressive) برای پیش‌بینی متن استفاده می‌کند.** این مدل بر اساس معماری **Transformer** ساخته شده است و قادر به انجام وظایف مختلف NLP مثل خلاصه‌سازی، ترجمه، بازیابی اطلاعات و تولید متن است.

---

## 📌 ۱. **معماری BART**

### الف) **Encoder**
- بر اساس **Transformer Encoder**
- نمایش متن ورودی را به یک بردار مفهومی تبدیل می‌کند.
- تمام توکن‌ها در یک زمان پردازش می‌شوند (غیر از masked tokens).

### ب) **Decoder**
- بر اساس **Transformer Decoder**
- از Masked Self-Attention استفاده می‌کند تا فقط اطلاعات قبلی را در نظر بگیرد (چپ به راست).
- همچنین از Encoder-Decoder Attention استفاده می‌کند تا اطلاعات از Encoder به Decoder منتقل شود.

### ج) **Positional Embeddings**
- برای تشخیص ترتیب توکن‌ها استفاده می‌شوند.
- BART از positional embeddings یادگیری‌شده استفاده می‌کند (نه ثابت).

### د) **Input/Output Embeddings**
- تمام ورودی‌ها و خروجی‌ها به بردارهای عددی تبدیل می‌شوند.
- BART از واژه‌نامه مشترک (shared vocabulary) 32,000 subword unit (WordPiece) استفاده می‌کند.

---

## 🧠 ۲. **هدف آموزشی BART: Denoising Objective**

BART روی داده‌های آموزشی بدون نظارت (pre-training) قرار می‌گیرد. هدف آن **بازیابی متن اصلی** است که در آن بخشی از متن به صورت تصادفی حذف یا جایگزین شده است. این روش شبیه به **Span Corruption** در T5 است.

### انواع Corruption:
1. **Replace corrupted spans**: یک قسمت از متن با `[MASK]` جایگزین می‌شود.
2. **Token Deletion**: بعضی از توکن‌ها حذف می‌شوند.
3. **Text Infilling**: جایگزینی فواصل بین کلمات با `[MASK]`.
4. **Sentence Permutation**: جملات به صورت تصادفی مرتب می‌شوند.
5. **Document Rotation**: یک قسمت تصادفی از متن به عنوان شروع متن در نظر گرفته می‌شود.

این روش به مدل کمک می‌کند تا ساختار متن را یاد بگیرد و بتواند در وظایف مختلف generalization کند.

---

## 🎯 ۳. **وظایف NLP در قالب Seq-to-Seq**

در BART، تمام وظایف به شکل زیر فرمول‌بندی می‌شوند:

```
[Input text] → [Output text]
```

مثال‌ها:

| وظیفه | مثال |
|-------|------|
| **ترجمه** | `translate English to German: That is good.` → `Das ist gut.` |
| **خلاصه‌سازی** | `summarize: state authorities dispatched emergency crews...` → `six people hospitalized after storm` |
| **طبقه‌بندی** | `classify sentiment: This movie was terrible.` → `negative` |
| **پاسخ‌دهی به سؤال** | `answer question: What is the capital of France?` → `Paris` |

---

## 🔁 ۴. **Fine-tuning BART**

بعد از pre-training، BART روی وظایف خاص fine-tune می‌شود. در این مرحله، تنها یک لایه linear یا چند لایه feed-forward روی خروجی decoder اضافه می‌شود.

### انواع Fine-tuning:
- **Full fine-tuning**: تمام پارامترهای مدل به‌روزرسانی می‌شوند.
- **Prompt-based tuning**: بدون تغییر وزن‌ها، از طریق prompt template عمل می‌شود.
- **Few-shot learning**: با تعداد کمی نمونه آموزشی (مثل 10 نمونه).
- **Zero-shot**: بدون هیچ نمونه آموزشی، فقط با توصیف وظیفه.

---

## 📈 ۵. **عملکرد BART در مقایسه با سایر مدل‌ها**

| مدل | GLUE (avg) | SQuAD | CNNDM | EnDe | EnFr | منبع |
|-----|------------|--------|--------|------|------|--------|
| **BART-base** | 84.1 | 90.8 | 25.05 | 27.07 | 40.65 | جزوه Section4 |
| **BART-large** | 84.8 | 91.4 | 25.12 | 27.21 | 41.22 | جزوه Section4 |
| **T5-base** | 82.7 | 92.1 | 20.3 | 30.9 | 41.2 | جزوه Section4 |
| **BERT-base** | 83.2 | 90.6 | 19.2 | — | — | جزوه Section4 |

> ⚠️ **نکته مهم:** BART در وظایف تولیدی (مانند خلاصه‌سازی و ترجمه) عملکرد بهتری نسبت به BERT دارد، ولی در وظایف classification ممکن است BERT بهتر عمل کند.

---

## 📦 ۶. **نسخه‌های مختلف BART**

| نسخه | تعداد پارامتر | تعداد لایه | بعد Hidden | FF Dimension | Head‌های Attention |
|------|----------------|--------------|---------------|------------------|--------------------|
| **BART-base** | 140 میلیون | 12 | 768 | 3072 | 12 |
| **BART-large** | 406 میلیون | 24 | 1024 | 4096 | 16 |

هر چه تعداد پارامترها بیشتر باشد، عملکرد بهتری در وظایف پیچیده دارد، اما نیازمند منابع محاسباتی بیشتری است.

---

## 📊 ۷. **نتایج عملکردی BART**

### الف) **مقایسه با روش‌های دیگر (در شرایط Low-data)**

| روش | SST-2 | CoLA | MRPC | QNLI | RTE |
|------|-------|------|------|------|-----|
| **Prompt-based Zero-shot** | 83.6% | 32.0% | 61.9% | 50.8% | 51.3% |
| **Prompt-based Few-shot** | 92.7% | 91.2% | 74.5% | 68.3% | 69.1% |
| **Full Fine-tuning** | 95.0% | 97.0% | 91.4% | 93.3% | 80.9% |

مشاهده می‌کنیم که Prompt-based tuning به خوبی می‌تواند بدون نیاز به fine-tuning کامل، عملکرد بالایی داشته باشد.

---

## 🔍 ۸. **مزایای BART**

- **Unified Framework**: تمام وظایف NLP در قالب "sequence-to-sequence" فرمول‌بندی می‌شوند.
- **قابلیت Generalization**: می‌تواند در وظایفی که در مرحله آموزش دیده نشده است (zero-shot) عملکرد خوبی داشته باشد.
- **انعطاف‌پذیری بالا**: می‌تواند به راحتی برای وظایف مختلف fine-tune شود.
- **کاربرد در زبان‌های مختلف**: با واژه‌نامه shared، می‌تواند در زبان‌های مختلف کار کند.

---

## 🧩 ۹. **معایب BART**

- **سنگینی مدل‌های بزرگ**: نسخه‌های large به منابع محاسباتی قوی نیاز دارند.
- **سرعت پایین در Inference**: به خصوص برای مدل‌های بزرگ.
- **عدم وجود NSP**: برخلاف BERT، BART از Next Sentence Prediction استفاده نمی‌کند.
- **نیاز به Prompt Engineering**: در روش‌های zero-shot/few-shot، طراحی دقیق prompt و label words بسیار مهم است.

---

## 🧪 ۱۰. **کاربردهای BART**

- **ترجمه ماشینی**
- **خلاصه‌سازی**
- **پاسخ‌دهی به سوال (Question Answering)**
- **تشخیص تشابه جمله (STS)**
- **Natural Language Inference (NLI)**
- **تشخیص موجودیت نام‌گذاری شده (NER)**
- **ویرایش متن و بازیابی اطلاعات**

---

## 🧠 نتیجه‌گیری

**BART یکی از قدرتمندترین مدل‌های زبانی است که تمام وظایف NLP را در قالب یک معماری واحد (seq-to-seq) حل می‌کند.** این مدل به خوبی می‌تواند در شرایط مختلف، اعم از zero-shot، few-shot و full fine-tuning، عملکرد خوبی داشته باشد. با این حال، استفاده از آن در شرایط منابع محدود ممکن است دشوار باشد.

---

اگر می‌خواهید نحوه استفاده از BART در پروژه‌های عملی (مثل fine-tuning با HuggingFace)، یا مقایسه با مدل‌های دیگر مثل T5، GPT یا RoBERTa را ببینید، بفرمایید!

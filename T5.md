### معماری و توضیحات کلی مدل **T5 (Text-to-Text Transfer Transformer)**

**T5 یک مدل زبانی عمومی است که تمام وظایف پردازش زبان طبیعی (NLP) را به صورت unified (متحد) در قالب "متن به متن" (text-to-text) فرمول‌بندی می‌کند.** این مدل بر اساس معماری **Encoder-Decoder Transformer** ساخته شده است و قادر است وظایف مختلفی مثل ترجمه، خلاصه‌سازی، پاسخ‌دهی به سؤال، طبقه‌بندی و غیره را با یک معماری واحد انجام دهد.

---

## 📌 ۱. **معماری T5**

### الف) **Encoder**
- بر اساس Transformer Encoder
- نمایش متن ورودی را به یک بردار مفهومی تبدیل می‌کند.
- اطلاعات متن ورودی را در حافظه مدل قرار می‌دهد.

### ب) **Decoder**
- بر اساس Transformer Decoder
- با استفاده از خروجی Encoder و توکن قبلی، خروجی مورد نظر را پیش‌بینی می‌کند.
- از Masked Self-Attention استفاده می‌کند تا فقط اطلاعات قبلی را در نظر بگیرد (چپ به راست).

### ج) **Positional Embeddings**
- برای تشخیص ترتیب توکن‌ها استفاده می‌شوند.
- T5 از positional embeddings یادگیری‌شده استفاده می‌کند (نه ثابت).

### د) **Input/Output Embeddings**
- تمام ورودی‌ها و خروجی‌ها به بردارهای عددی تبدیل می‌شوند.
- T5 از واژه‌نامه مشترک (shared vocabulary) 32,000 subword unit (WordPiece) استفاده می‌کند.

---

## 🧠 ۲. **هدف آموزشی T5: Denoising Objective**

T5 ابتدا روی داده‌های آموزشی بدون نظارت (pre-training) قرار می‌گیرد. هدف آن **بازیابی متن اصلی** است که در آن بخشی از متن به صورت تصادفی حذف یا جایگزین شده است. این روش شبیه به **Span Corruption** در BERT است.

### انواع Corruption:
1. **Replace corrupted spans**: یک قسمت از متن با `[MASK]` جایگزین می‌شود.
2. **Drop corrupted tokens**: بعضی از توکن‌ها حذف می‌شوند.
3. **Shuffling sentences**: جملات به صورت تصادفی مرتب می‌شوند.
4. **Document rotation**: یک قسمت تصادفی از متن به عنوان شروع متن در نظر گرفته می‌شود.

این روش به مدل کمک می‌کند تا ساختار متن را یاد بگیرد و بتواند در وظایف مختلف generalization کند.

---

## 🎯 ۳. **وظایف NLP در قالب Text-to-Text**

در T5، تمام وظایف به شکل زیر فرمول‌بندی می‌شوند:

```
[Task-specific prefix]: [Input text] -> [Output text]
```

مثال‌ها:

| وظیفه | مثال |
|-------|------|
| **ترجمه** | `translate English to German: That is good.` → `Das ist gut.` |
| **خلاصه‌سازی** | `summarize: state authorities dispatched emergency crews...` → `six people hospitalized after storm` |
| **طبقه‌بندی** | `classify sentiment: This movie was terrible.` → `negative` |
| **پاسخ‌دهی به سوال** | `answer question: What is the capital of France?` → `Paris` |

---

## 🔁 ۴. **Fine-tuning T5**

بعد از pre-training، T5 روی وظایف خاص fine-tune می‌شود. در این مرحله، تنها یک لایه linear یا چند لایه feed-forward روی خروجی decoder اضافه می‌شود.

### انواع Fine-tuning:
- **Full fine-tuning**: تمام پارامترهای مدل به‌روزرسانی می‌شوند.
- **Prompt-based tuning**: بدون تغییر وزن‌ها، از طریق prompt template عمل می‌شود.
- **Few-shot learning**: با تعداد کمی نمونه آموزشی (مثل 10 نمونه).
- **Zero-shot**: بدون هیچ نمونه آموزشی، فقط با توصیف وظیفه.

---

## 📈 ۵. **عملکرد T5 در مقایسه با سایر مدل‌ها**

| مدل | GLUE (avg) | SQuAD | CNNDM | EnDe | EnFr | منبع |
|-----|------------|--------|--------|------|------|--------|
| **T5-small** | 77.4 | 87.2 | 19.6 | 26.7 | 36.0 | جزوه Section4 |
| **T5-base** | 82.7 | 92.1 | 20.3 | 30.9 | 41.2 | جزوه Section4 |
| **T5-large** | 86.4 | 93.8 | 20.7 | 32.0 | 41.5 | جزوه Section4 |
| **T5-3B** | 88.5 | 94.9 | 21.0 | 31.8 | 42.6 | جزوه Section4 |
| **T5-11B** | 89.7 | 95.6 | 21.5 | 32.1 | 43.4 | جزوه Section4 |
| **BERT-base** | 83.2 | 90.6 | 19.2 | — | — | جزوه Section4 |

> ⚠️ **نکته مهم:** T5 در وظایف تولیدی (مانند خلاصه‌سازی و ترجمه) عملکرد بهتری نسبت به BERT دارد، ولی در وظایف classification ممکن است BERT بهتر عمل کند.

---

## 📦 ۶. **نسخه‌های مختلف T5**

| نسخه | تعداد پارامتر | تعداد لایه | بعد Hidden | FF Dimension | Head‌های Attention |
|------|----------------|--------------|---------------|------------------|--------------------|
| **T5-Small** | 60 میلیون | 6 | 512 | 2048 | 8 |
| **T5-Base** | 220 میلیون | 12 | 768 | 3072 | 12 |
| **T5-Large** | 770 میلیون | 24 | 1024 | 4096 | 16 |
| **T5-3B** | 3 میلیارد | 24 | 1024 | 16384 | 32 |
| **T5-11B** | 11 میلیارد | 24 | 1024 | 65536 | 128 |

هر چه تعداد پارامترها بیشتر باشد، عملکرد بهتری در وظایف پیچیده دارد، اما نیازمند منابع محاسباتی بیشتری است.

---

## 📊 ۷. **نتایج عملکردی T5**

### الف) **مقایسه با روش‌های دیگر (در شرایط Low-data)**

| روش | SST-2 | CoLA | MRPC | QNLI | RTE |
|------|-------|------|------|------|-----|
| **Prompt-based Zero-shot** | 83.6% | 32.0% | 61.9% | 50.8% | 51.3% |
| **Prompt-based Few-shot** | 92.7% | 91.2% | 74.5% | 68.3% | 69.1% |
| **Full Fine-tuning** | 95.0% | 97.0% | 91.4% | 93.3% | 80.9% |

مشاهده می‌کنیم که Prompt-based tuning به خوبی می‌تواند بدون نیاز به fine-tuning کامل، عملکرد بالایی داشته باشد.

---

## 🔍 ۸. **مزایای T5**

- **Unified Framework**: تمام وظایف NLP در قالب "متن به متن".
- **قابلیت Generalization**: می‌تواند در وظایفی که در مرحله آموزش دیده نشده است (zero-shot) عملکرد خوبی داشته باشد.
- **انعطاف‌پذیری بالا**: می‌تواند به راحتی برای وظایف مختلف fine-tune شود.
- **کاربرد در زبان‌های مختلف**: با واژه‌نامه shared، می‌تواند در زبان‌های مختلف کار کند.

---

## 🧩 ۹. **معایب T5**

- **سنگینی مدل‌های بزرگ**: نسخه‌های 3B و 11B به منابع محاسباتی قوی نیاز دارند.
- **سرعت پایین در Inference**: به خصوص برای مدل‌های بزرگ.
- **عدم وجود NSP**: برخلاف BERT، T5 از Next Sentence Prediction استفاده نمی‌کند.
- **نیاز به Prompt Engineering**: در روش‌های zero-shot/few-shot، طراحی دقیق prompt و label words بسیار مهم است.

---

## 🧪 ۱۰. **کاربردهای T5**

- **ترجمه ماشینی**
- **خلاصه‌سازی**
- **پاسخ‌دهی به سوال (Question Answering)**
- **تشخیص تشابه جمله (STS)**
- **Natural Language Inference (NLI)**
- **تشخیص موجودیت نام‌گذاری شده (NER)**
- **ویرایش متن و بازیابی اطلاعات**

---

## 🧠 نتیجه‌گیری

**T5 یکی از قدرتمندترین مدل‌های زبانی است که تمام وظایف NLP را در قالب یک معماری واحد (text-to-text) حل می‌کند.** این مدل به خوبی می‌تواند در شرایط مختلف، اعم از zero-shot، few-shot و full fine-tuning، عملکرد خوبی داشته باشد. با این حال، استفاده از آن در شرایط منابع محدود ممکن است دشوار باشد.

---

اگر می‌خواهید نحوه استفاده از T5 در پروژه‌های عملی (مثل fine-tuning با HuggingFace)، یا مقایسه با مدل‌های دیگر مثل BART، GPT یا RoBERTa را ببینید، بفرمایید!
